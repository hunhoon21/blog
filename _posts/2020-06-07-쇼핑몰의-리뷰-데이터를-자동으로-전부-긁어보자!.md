## ì‹œì‘í•˜ë©°

ì´ë²ˆ ê¸€ì—ì„œëŠ” 11ë²ˆê°€ì—ì„œ ìƒí’ˆì— ëŒ€í•œ ë¦¬ë·° í…ìŠ¤íŠ¸ ë°ì´í„°ì™€ ê·¸ ìƒí’ˆì— ëŒ€í•œ í‰ì ì„ ê¸ì€ í¬ë¡¤ë§ ì‘ì—… ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì°¸ê³ ë¡œ, í™ˆí˜ì´ì§€ì— ì¡´ì¬í•˜ëŠ” (ê±°ì˜) ëª¨ë“  ë°ì´í„°ë¥¼ ê¸ëŠ” ê²ƒì´ ëª©í‘œì˜€ìŠµë‹ˆë‹¤. (ã…ã… ê±°ì°½í•˜ì£ ?) ì½”ë“œ ì‘ì„±ì˜ ì‚¬ê³  íë¦„ê³¼ ë¬¸ì œ í•´ê²° ê³¼ì •ì— ëŒ€í•´ ì´ì•¼ê¸° í• ê¹Œ í•©ë‹ˆë‹¤.

### ê¸°ëŒ€ ë…ì

1. ê¸°ë³¸ì ì¸ HTML êµ¬ì¡°ì™€ CSS ë¬¸ë²•ì— ëŒ€í•´ ì•Œë©´ ì¢‹ìŠµë‹ˆë‹¤. ì•„ë˜ë¥¼ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤.
   1. [CSS Selector Reference](https://www.w3schools.com/cssref/css_selectors.asp)
2. `BeautifulSoup` ê³¼ `Selenium` ì„ ì´ìš©í•˜ì—¬ í˜ì´ì§€ë‚´ í•„ìš”í•œ ìš”ì†Œë¥¼ ë½‘ì•„ ë‚¼ ìˆ˜ ìˆìœ¼ë©´ ì¢‹ìŠµë‹ˆë‹¤.
   1. [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
   2. [WebDriverë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ìš”ì†Œ ì°¾ê¸°]([https://riptutorial.com/ko/selenium-webdriver/example/13934/webdriver%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC-%ED%8E%98%EC%9D%B4%EC%A7%80-%EC%9A%94%EC%86%8C-%EC%B0%BE%EA%B8%B0](https://riptutorial.com/ko/selenium-webdriver/example/13934/webdriverë¥¼-ì‚¬ìš©í•˜ì—¬-í˜ì´ì§€-ìš”ì†Œ-ì°¾ê¸°))
3. í¬ë¡¤ë§ ê³¼ì •ì„ í•œë²ˆ ì‚´í´ë³´ê³  ì‹¶ì€ ì‚¬ëŒ
4. ì‹¬ì‹¬í•œ ì‚¬ëŒ



## í¬ë¡¤ë§ì˜ ì „ì²´ì ì¸ ê³¼ì •ê³¼ ìµœì¢… ì‚°ì¶œë¬¼

í¬ë¡¤ë§ì„ ì–´ë–¤ ê³¼ì •ìœ¼ë¡œ ì§„í–‰í–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ì œê°€ ë½‘ê³ ì í–ˆë˜ ë°ì´í„°ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¸ì§€ ê²°ê³¼ ì•„ì›ƒí’‹ì˜ ì¼ë¶€ë¥¼ ë³´ì—¬ë“œë¦´ê²Œìš”.

### ì „ì²´ì ì¸ ê³¼ì • íë¦„

1. í˜„ì¬ì˜ í˜ì´ì§€ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì „ë¶€ ê¸ì–´ì˜µë‹ˆë‹¤.
2. ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
3. ë í˜ì´ì§€ì´ê³ , ë‹¤ìŒ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
4. ìœ„ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
5. í•´ë‹¹ URLì—ì„œ ìœ„ ê³¼ì •ì„ ëëƒˆìœ¼ë©´ ë‹¤ìŒ URLìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.

> ![crawling](../imgs/crawl_11st_1.gif)

### ìµœì¢… ì‚°ì¶œë¬¼

ì•„ë˜ì˜ í•­ëª©ë“¤ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.

* ì œí’ˆëª…
* ì œí’ˆ ë²ˆí˜¸
* ì¹´í…Œê³ ë¦¬ ì •ë³´
* êµ¬ë§¤ ì˜µì…˜
* ë¦¬ë·° í…ìŠ¤íŠ¸
* ë³„ì 
* íƒœê·¸
* ì‘ì„± ë‚ ì§œ
* ì¢‹ì•„ìš” ê°¯ìˆ˜
* ì‹«ì–´ìš” ê°¯ìˆ˜

> ![crawl_1](../imgs/crawl_1.png)

ë°ì´í„°ëŠ” ì§€ê¸ˆê¹Œì§€ ì•½ 770ë§Œì—¬ê°œê°€ ìˆ˜ì§‘ë˜ì—ˆê³ , ë³„ì  ê¸°ì¤€ìœ¼ë¡œ ë´¤ì„ë•Œ ê°ê°ì˜ ê°¯ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë„¤ìš”.

> ![crawl_2](../imgs/crawl_2.png)

## í¬ë¡¤ë§ ê³¼ì •ê³¼ íŒŒì´ì¬ ì½”ë“œ



### 0. í¬ë¡¤ë§ ì¤€ë¹„

ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì›¹ë“œë¼ì´ë²„ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ì˜ˆì‹œì—ì„œëŠ” [ì‹¸ì´ë‹‰, ë‚´ê²Œ ë”± ë§ëŠ” ì¬! ê³¨ë¼ë³´ì¬!](http://www.11st.co.kr/product/SellerProductDetail.tmall?method=getSellerProductDetail&prdNo=1242750805&trTypeCd=21&trCtgrNo=1002065) ì œí’ˆ urlì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•´ë‹¹ urlë¡œ ì ‘ì†í•©ë‹ˆë‹¤.

```python
import time
import requests
from typing import List, Tuple
import pickle
from json import dump

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException


chrome_driver_path = '/Users/henris/chromedriver'  
driver = webdriver.Chrome(chrome_driver_path)
url = 'http://www.11st.co.kr/product/SellerProductDetail.tmall?method=getSellerProductDetail&prdNo=1242750805&trTypeCd=21&trCtgrNo=1002065'

driver.get(url)
WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'ifrmReview')))
```

### 1. í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì´ˆ ë°ì´í„°(ì œí’ˆëª…, ì¹´í…Œê³ ë¦¬)

`driver` ë¡œ ì ‘ê·¼í•´ ì œí’ˆëª…ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

```python
def get_title(driver) -> str:
    soup = BeautifulSoup(driver.page_source)
    
    title = soup.select_one('h2').get_text()
    return title.strip()


def get_cat_info(driver) -> Tuple[str, str, str]:
    soup = BeautifulSoup(driver.page_source)
    
    cat_info_1 = soup.select_one('#headSel_1').get_text()
    cat_info_2 = soup.select_one('#headSel_2').get_text()
    cat_info_3 = soup.select_one('#headSel_3').get_text()
    return cat_info_1.strip(), cat_info_2.strip(), cat_info_3.strip()


title = get_title(driver)
cat_info_1, cat_info_2, cat_info_3 = get_cat_info(driver)
```



### 2. í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ë·° ë°ì´í„° ê¸ê¸°

ì‚¬ì‹¤ CSS Selectorë‚˜ HTML ë¬¸ë²•ìœ¼ë¡œ ë¦¬ë·° ë°ì´í„°ì— ë°”ë¡œ ì ‘ê·¼ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¦¬ë·° ì„¹ì…˜ì€ ì•„ì´í”„ë ˆì„ êµ¬ì¡°ë¡œ ì´ë£¨ì–´ì ¸ìˆê¸° ë•Œë¬¸ì¸ë°ìš”.(ì‚½ì§ˆì˜ ì›ì¸... ğŸ¤£) ë”°ë¼ì„œ ë°”ê¿”ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.

```python
# review section is iframe structure, need to change
driver.switch_to.frame("ifrmReview")
WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'ifrm_prdc_review')))
```

ì´ì œ ë§ˆìŒê» íŒŒì‹±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ :) 

```python
def get_html_with_bs(driver) -> BeautifulSoup:
    soup = BeautifulSoup(driver.page_source)
    return soup

  
def list_review_on_this_page(soup) -> BeautifulSoup:
    return soup.find('div', class_='review_list').find_all('li')
  
  
# whole review information from this page
soup_on_this_page = get_html_with_bs(driver)
reviews_on_this_page = list_review_on_this_page(soup_on_this_page)
```

ì´ì œ í•´ë‹¹ í•œ í˜ì´ì§€ì˜ ë¦¬ë·° ë°ì´í„°ë¥¼ ì–»ê¸° ìœ„í•œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤. íƒœê·¸, í…ìŠ¤íŠ¸, ë³„ì  ë“±ì˜ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì•¼ê² ì£ ?

```python
def get_review_info(soup) -> BeautifulSoup:
    return soup.find('a', class_='reviewDetail')


def link_to_review_detail_page(
    contMapNo, prdNo, contNo
) -> BeautifulSoup:
    review_page = REVIEW_PAGE_FMT.format(
        contMapNo=contMapNo, prdNo=prdNo, contNo=contNo
    )
    resp = requests.get(review_page)
    soup = BeautifulSoup(resp.text)
    return soup


def get_tags_of_review(soup) -> List[str]:
    return [
        tag.get_text()
        for tag in soup.find_all('span', class_='re_ico')
    ]


def get_text_of_review(soup) -> str:
    text = soup.find('div', class_='bbs_cont_wrap').get_text()
    text = text.replace(STUPID_TEXT_1, '')
    text = text.replace(STUPID_TEXT_2, '')
    text = text.replace(STUPID_TEXT_3, '')
    text = text.replace(STUPID_TEXT_4, '')
    return text.strip()


def get_stars_of_review(soup) -> int:
    return int(
        soup.find('span', class_='selr_star').get_text()[-2]
    )

def get_option_of_review(soup) -> str:
    return soup.select_one('p.option_txt').get_text().strip()


def get_date_of_review(soup) -> str:
    return soup.select_one('span.date').get_text().strip()


def get_good_and_bad_of_review(soup) -> Tuple[int, int]:
    good = soup.select_one('a.good span.cnt').get_text().strip()
    bad = soup.select_one('a.bad span.cnt').get_text().strip()
    return int(good), int(bad)


REVIEW_PAGE_FMT = \
'http://deal.11st.co.kr/product/SellerProductDetail.tmall?method=\
getProductReviewDetailv2&contMapNo={contMapNo}&prdNo={prdNo}&contNo={contNo}'

STUPID_TEXT_1 = 'ì¸í„°ë„· ìµìŠ¤í”Œë¡œëŸ¬ í•˜ìœ„ë²„ì „ì„ ì‚¬ìš©í•˜ê³  ìˆì–´ ë™ì˜ìƒì´ ë…¸ì¶œì´ ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.'
STUPID_TEXT_2 = 'ìµœì„ ë²„ì „ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.'
STUPID_TEXT_3 = 'ìµìŠ¤í”Œë¡œëŸ¬ ë‹¤ìš´ë¡œë“œë°”ë¡œê°€ê¸°'
STUPID_TEXT_4 = 'ë™ì˜ìƒì¬ìƒ'
```

íƒœê·¸ ì •ë³´ë¥¼ ê°€ì§€ê³  ì˜¤ëŠ” ê²ƒì€ ì‚¬ì§„ì„ ì²¨ë¶€í•œ detailed reviewë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì €ëŠ” íƒœê·¸ë„ ê°€ì§€ê³  ì˜¤ê¸° ìœ„í•´ ì‚¬ì§„ì„ ì²¨ë¶€í•˜ì§€ ì•Šì€ ê¸€ë§Œ ì¡´ì¬í•˜ëŠ” ë¦¬ë·°ëŠ” ì œì™¸í–ˆìŠµë‹ˆë‹¤.

```python
# get information to link a review detail page
review_info = get_review_info(review_on_this_page)
if not review_info:
    continue
contMapNo = review_info['data-contmapno']
prdNo = review_info['data-prdno']
contNo = review_info['data-contno']

review_detail_page = link_to_review_detail_page(
    contMapNo=contMapNo, prdNo=prdNo, contNo=contNo
)

# extract information from the review detail page
tags = get_tags_of_review(review_detail_page)
text = get_text_of_review(review_detail_page)
stars = get_stars_of_review(review_detail_page)
option = get_option_of_review(review_detail_page)
date = get_date_of_review(review_detail_page)
good, bad = get_good_and_bad_of_review(review_detail_page)

# save specific review information
if text:
    reviews.append(
        {
            'title': title,
            'prdNo': prdNo,
            'catInfo1': cat_info_1,
            'catInfo2': cat_info_2,
            'catInfo3': cat_info_3,
            'option': option,
            'tags': tags,
            'text': text,
            'stars': stars,
            'date': date,
            'good': good,
            'bad': bad,
        }
    )

print('í˜„ì¬ {}ë²ˆ page í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'.format(page_num))
```



### 3. ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°€ê¸°

`try/except` êµ¬ë¬¸ì„ í™œìš©í•˜ì—¬ ë‹¤ìŒí˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.

```python
def go_to_next_page(driver, page_num) -> None:
  next_page = driver.find_element_by_css_selector('div.review_list strong ~ #paging_page')
  next_page.click()


# go to next page (ex: 1->2 or 16->17)
try:
    go_to_next_page(driver, page_num)
    print('ë‹¤ìŒ pageë¡œ ë„˜ì–´ê°€ëŠ” ì¤‘ì…ë‹ˆë‹¤.....\n')
    page_num += 1
    continue      
except NoSuchElementException:
    print('ì´ë²ˆ sectionì—ì„œëŠ” ë§ˆì§€ë§‰ page ì…ë‹ˆë‹¤.')
```



### 4. ë í˜ì´ì§€ì´ê³ , ë‹¤ìŒ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ë„˜ì–´ê°€ê¸°

ë í˜ì´ì§€(ex 10ë²ˆ í˜ì´ì§€, 30ë²ˆ í˜ì´ì§€) ì˜€ë‹¤ë©´ 3ë²ˆì˜ except ì— ê±¸ë¦¬ê²Œ ë˜ê³ , ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” êµ¬ë¬¸ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.

```python
def go_to_next_section(driver) -> None:
    next_section = driver.find_element_by_css_selector('.review_list #paging_next')
    next_section.click()
    
    
# go to next section (ex: 10->11 or 30->31)
try:
    go_to_next_section(driver)
    print('ë‹¤ìŒ sectionìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ì¤‘ì…ë‹ˆë‹¤.....\n')
    page_num += 1
except NoSuchElementException:
    print('ì´ê²ƒìœ¼ë¡œ í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.')
    break
```



### 5. URL ì •ë³´ ëª¨ì•„ì˜¤ê¸°

ì´ì œ í•œ ì œí’ˆì˜ ëª¨ë“  ë¦¬ë·° ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì‘ì—…ê¹Œì§€ ëë‚¬ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì™„ì „ ìë™í™”ë¥¼ ì´ë£¨ê¸° ìœ„í•´ì„œëŠ” URL ì •ë³´ê°€ ë¬´ìˆ˜íˆ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.  

URL ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” ì•½ê°„ì˜ ì”ë¨¸ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ë¡œ ì ‘ê·¼í•˜ì—¬ ë°ì´í„°ë¥¼ ì–»ì€ ê²½ìš° URL í¬ë§·ì´ ì•„ë˜ì˜ ê²½ìš°ì™€ ê°™ì•˜ìŠµë‹ˆë‹¤. ë˜í•œ ì–´ë–¤ ìˆ«ìì˜ ê²½ìš°ëŠ” 100 ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” 7ìë¦¬ì˜ ìˆ«ìë”êµ°ìš”.

> http://www.11st.co.kr/category/DisplayCategory.tmall?method=getDisplayCategory2Depth&&dispCtgrNo={ì–´ë–¤ ìˆ«ì}#sortCd%%I$$pageNum%%1

ê·¸ëŸ¼ ìš°ë¦¬ëŠ” ìœ„ URLì— 7ìë¦¬ ìˆ«ìë¥¼ ë„£ì–´ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¥¼ í™•ì¸í•´ë³´ê³ , ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ ê±°ê¸°ì„œ ë¦¬ë·° ë§ì€ ìˆœìœ¼ë¡œ ë§í¬ URLì„ ê°€ì ¸ì™€ë³´ì£ !

```python
driver = webdriver.Chrome(chrome_driver_path)
URL_FMT = 'http://www.11st.co.kr/category/DisplayCategory.tmall?method=getDisplayCategory2Depth&&dispCtgrNo={CtgrNo}#sortCd%%I$$pageNum%%1'

ctgr_no = 1001301
total_list = []

while True:
    driver.get(URL_FMT.format(CtgrNo=ctgr_no))
    try:
        WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.CSS_SELECTOR, '#product_listing .total_listing_wrap')))
        
    except Exception as e:
        ctgr_no = ctgr_no - (ctgr_no % 10) + 11
        continue
    keywords = driver.find_element_by_css_selector('meta[name=keywords]').get_attribute('content')
    this_page_soup = BeautifulSoup(driver.page_source)
    item_soup_list = this_page_soup.select('#product_listing .total_listitem')
    print(len(item_soup_list))
    this_keyword_value = []
    for item_soup in item_soup_list:
        price = item_soup.select_one('.sale_price').get_text()
        link = item_soup.select_one('a[href]')['href']
        this_keyword_value.append(
            {
                'link': link,
                'price': price
            }
        )
    total_list.append(this_keyword_value)
    ctgr_no += 1
    
    if ctgr_no > 2000000:
        break
    
with open('list_11st_link.pkl', 'wb') as f:
    pickle.dump(total_list, f)
```

>  ![crawl_4](../imgs/crawl_4.png)

ë§í¬ê°€ ë§ì´ ìŒ“ì˜€ë„¤ìš” :) 

## í¬ë¡¤ë§ ìë™í™” íŒŒì´ì¬ ì½”ë“œ

ìœ„ ê³¼ì •ì„ ë¬¶ì–´ì„œ ë°ì´í„°ë¥¼ ê³„ì† ìˆ˜ì§‘í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë©° ê¸€ì„ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

```python
import time
import requests
from typing import List, Tuple
import pickle
from json import dump

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException


def get_cat_info(driver) -> Tuple[str, str, str]:
    soup = BeautifulSoup(driver.page_source)
    
    cat_info_1 = soup.select_one('#headSel_1').get_text()
    cat_info_2 = soup.select_one('#headSel_2').get_text()
    cat_info_3 = soup.select_one('#headSel_3').get_text()
    return cat_info_1.strip(), cat_info_2.strip(), cat_info_3.strip()


def get_title(driver) -> str:
    soup = BeautifulSoup(driver.page_source)
    
    title = soup.select_one('h2').get_text()
    return title.strip()


def go_to_next_section(driver) -> None:
    next_section = driver.find_element_by_css_selector('.review_list #paging_next')
    next_section.click()


def go_to_next_page(driver, page_num) -> None:
    next_page = driver.find_element_by_css_selector('div.review_list strong ~ #paging_page')
    next_page.click()


def get_html_with_bs(driver) -> BeautifulSoup:
    soup = BeautifulSoup(driver.page_source)
    return soup


def list_review_on_this_page(soup) -> BeautifulSoup:
    return soup.find('div', class_='review_list').find_all('li')


def get_review_info(soup) -> BeautifulSoup:
    return soup.find('a', class_='reviewDetail')


def link_to_review_detail_page(
    contMapNo, prdNo, contNo
) -> BeautifulSoup:
    review_page = REVIEW_PAGE_FMT.format(
        contMapNo=contMapNo, prdNo=prdNo, contNo=contNo
    )
    resp = requests.get(review_page)
    soup = BeautifulSoup(resp.text)
    return soup


def get_tags_of_review(soup) -> List[str]:
    return [
        tag.get_text()
        for tag in soup.find_all('span', class_='re_ico')
    ]


def get_text_of_review(soup) -> str:
    text = soup.find('div', class_='bbs_cont_wrap').get_text()
    text = text.replace(STUPID_TEXT_1, '')
    text = text.replace(STUPID_TEXT_2, '')
    text = text.replace(STUPID_TEXT_3, '')
    text = text.replace(STUPID_TEXT_4, '')
    return text.strip()


def get_stars_of_review(soup) -> int:
    return int(
        soup.find('span', class_='selr_star').get_text()[-2]
    )

def get_option_of_review(soup) -> str:
    return soup.select_one('p.option_txt').get_text().strip()


def get_date_of_review(soup) -> str:
    return soup.select_one('span.date').get_text().strip()


def get_good_and_bad_of_review(soup) -> Tuple[int, int]:
    good = soup.select_one('a.good span.cnt').get_text().strip()
    bad = soup.select_one('a.bad span.cnt').get_text().strip()
    return int(good), int(bad)


REVIEW_PAGE_FMT = \
'http://deal.11st.co.kr/product/SellerProductDetail.tmall?method=\
getProductReviewDetailv2&contMapNo={contMapNo}&prdNo={prdNo}&contNo={contNo}'

STUPID_TEXT_1 = 'ì¸í„°ë„· ìµìŠ¤í”Œë¡œëŸ¬ í•˜ìœ„ë²„ì „ì„ ì‚¬ìš©í•˜ê³  ìˆì–´ ë™ì˜ìƒì´ ë…¸ì¶œì´ ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.'
STUPID_TEXT_2 = 'ìµœì„ ë²„ì „ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.'
STUPID_TEXT_3 = 'ìµìŠ¤í”Œë¡œëŸ¬ ë‹¤ìš´ë¡œë“œë°”ë¡œê°€ê¸°'
STUPID_TEXT_4 = 'ë™ì˜ìƒì¬ìƒ'


chrome_driver_path = '/Users/henris/chromedriver'
driver = webdriver.Chrome(chrome_driver_path)

with open('list_11st_link.pkl', 'rb') as f:
    cat_url_list = pickle.load(f)
    
json_no = 0

for cat in cat_url_list:
    for info in cat:
        url = info['link']
        try:
            driver.get(url)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'ifrmReview')))
            title = get_title(driver)
            cat_info_1, cat_info_2, cat_info_3 = get_cat_info(driver)

            # review section is iframe structure, need to change
            driver.switch_to.frame("ifrmReview")
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'ifrm_prdc_review')))

            reviews = []

            page_num = 1

            while True:

                print('í˜„ì¬ {}ë²ˆ pageë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.'.format(page_num))

                # whole review information from this page
                soup_on_this_page = get_html_with_bs(driver)
                reviews_on_this_page = list_review_on_this_page(soup_on_this_page)

                for review_on_this_page in reviews_on_this_page:    
                    # get information to link a review detail page
                    review_info = get_review_info(review_on_this_page)
                    if not review_info:
                        print('í•´ë‹¹ ë¦¬ë·°ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•˜ë¯€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.')
                        continue
                    contMapNo = review_info['data-contmapno']
                    prdNo = review_info['data-prdno']
                    contNo = review_info['data-contno']

                    review_detail_page = link_to_review_detail_page(
                        contMapNo=contMapNo, prdNo=prdNo, contNo=contNo
                    )

                    # extract information from the review detail page
                    tags = get_tags_of_review(review_detail_page)
                    text = get_text_of_review(review_detail_page)
                    stars = get_stars_of_review(review_detail_page)
                    option = get_option_of_review(review_detail_page)
                    date = get_date_of_review(review_detail_page)
                    good, bad = get_good_and_bad_of_review(review_detail_page)

                    # save specific review information
                    if text:
                        reviews.append(
                            {
                                'title': title,
                                'prdNo': prdNo,
                                'catInfo1': cat_info_1,
                                'catInfo2': cat_info_2,
                                'catInfo3': cat_info_3,
                                'option': option,
                                'tags': tags,
                                'text': text,
                                'stars': stars,
                                'date': date,
                                'good': good,
                                'bad': bad,
                            }
                        )

                print('í˜„ì¬ {}ë²ˆ page í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'.format(page_num))

                # go to next page (ex: 1->2 or 16->17)
                try:
                    # ì œëŒ€ë¡œ ë„˜ì–´ê°€ì§€ ì•Šì€ ê²½ìš° wait êµ¬í˜„ í•„ìš”
                    go_to_next_page(driver, page_num)
                    print('ë‹¤ìŒ pageë¡œ ë„˜ì–´ê°€ëŠ” ì¤‘ì…ë‹ˆë‹¤.....\n')
                    page_num += 1
                    continue      
                except NoSuchElementException:
                    print('ì´ë²ˆ sectionì—ì„œëŠ” ë§ˆì§€ë§‰ page ì…ë‹ˆë‹¤.')

                # go to next section (ex: 10->11 or 30->31)
                try:
                    go_to_next_section(driver)
                    print('ë‹¤ìŒ sectionìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ì¤‘ì…ë‹ˆë‹¤.....\n')
                    page_num += 1
                except NoSuchElementException:
                    print('ì´ê²ƒìœ¼ë¡œ í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.')
                    break

            with open('{}.txt'.format(json_no), 'w') as f:
                dump(reviews, f)reviews
            json_no += 1

        except:
            print('ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. url - {}'.format(url))
            continue
```





